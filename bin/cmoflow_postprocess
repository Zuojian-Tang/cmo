#!/opt/common/CentOS_6-dev/python/python-2.7.10/bin/python
from cmo import workflow
import argparse, os, sys
import cmo 

def filter_hap_job(input, pairing_file, output_filename, output_dir):
    output_file = os.path.join(output_dir, output_filename)
    #FIXME TEMP DIRECTORY?!?!?!?
    haplo_cmd = ["cmo_filter_haplotype", "--pairing-file", pairing_file, "--haplotype-vcf", haplotype_vcf, "--output-file", hapmaf_filename]
    return workflow.Job(" ".join(cmd),name="Filt.Haplotype")

def filter_mutect_job(input, pairing, output_filename):
    cmd = ["cmo_filter_mutect", "--pairing-file", pairing_file, "--mutect-vcf", os.path.abspath(vcf), "--output-file", output_file]
    mutect_cmds.append(cmd)
    return (workflow.Job(" ".join(cmd), name="Filt.Mutect"), output_file)

def merged_maf_job(maf_inputs, merged_maf_name):
    merge_mafs_cmd = ["cmo_merge_mafs", " ".join(maf_inputs), "--output-file", merged_maf_name]
    return workflow.Job(" ".join(merge_mafs_cmd), name="Merge Mafs")

def trinuc_and_impact_job(merged_maf_name, seq_output, impact_outputoutput_dir):
    trinuc_cmd = ["cmo_trinuc_and_impact" , "--source-file", merged_maf_name, 
            "--genome", "hg19", 
            "--output-seq", merged_seq, 
            "--output-impact", merged_impact_pos]
    return workflow.Job(" ".join(trinuc_cmd), name="Trinuc and Impact")

def maf2maf_job(merged_maf_name, output_file):
    maf2maf_cmd = ['cmo_maf2maf', '--version=develop', '--vep-forks 12', 
            '--vep-path', cmo.util.programs['vep']['default'], '--vep-data', 
            cmo.util.programs['vep']['default'], '--ref-fasta', cmo.util.genomes['hg19']['fasta'],
            '--retain-cols', 
            'Center,Verification_Status,Validation_Status,Mutation_Status,Sequencing_Phase,Sequence_Source,Validation_Method,Score,BAM_file,Sequencer,Tumor_Sample_UUID,Matched_Norm_Sample_UUID,Caller',
            '--custom-enst', os.path.join(cmo.util.programs['vcf2maf']['develop'], "data", "isoform_overrides_at_mskcc"),
            '--input-maf', merged_maf_name,
            '--output-maf', output_file
            ]
    return workflow.Job(" ".join(maf2maf_cmd), name="maf2maf")

def add_variant_info(seq_input, impact_pos_input, orig_maf, output_file):
    add_var_info_cmd = ['cmo_add_variant+info', '--sequence-data-file', merged_seq, '--impact-positions', merged_impact_pos,'--original-maf', maf_w_vep]
    return workflow.job(" ".join(add_var_info_cmd), name="Add Variant Info")



def main(pairing_file, pipeline_output_dir, project_id, output_dir):
    pipeline_output_dir = os.path.abspath(pipeline_output_dir)
    haplotype_vcf_pattern = os.path.join(pipeline_output_dir, "variants",  "haplotype_caller", "*_HaplotypeCaller.vcf")
    if not project_id:
        m = re.match("(Proj_\d+(?:_\s)?", pipeline_output_dir)
        if not m:
            print >>sys.stderr, "Unable to find a PROJ_[NUMBER]_[A-Z] string in pipeline dir name"
            print >>sys.stderr, "Supply a PROJECT ID at command line to run with this dir."
            print >>sys.stderr, "Bailing out!"
            sys.exit(1)
    project_id = m.group(1)
    try:
        haplotype_vcf = glob.glob(haplotype_vcf_pattern)[0]
    except:
        print >>sys.stderr, "No Haplotype VCF found with %s glob pattern" % haplotype_vcf_pattern
    ##########MAKE JOBS#############################
    ####HAPMAF OUTPUT NAME
    hapmaf_filename = project_id + "___qSomHC_InDels__TCGA_MAF.txt"
    #HAPMAF JOB
    haplo_filter_job = filter_hap_job(haplotype_vcf, pairing_file, hapmaf_filename, output_dir)
    mutect_dir = os.path.join(pipeline_output_dir, "variants", "mutect", "")
    mutect_vcfs = glob.glob(os.path.join(mutect_dir, "*.vcf"))
    mutect_jobs = list()
    maf_outputs = [hapmaf_filename]
    #MAKE ALL MUTECT OUTPUTS AND JOBS
    for vcf in mutect_vcfs:
        output_file = os.path.basename(vcf.replace(".vcf", ".DMP_FILTER.maf"))
        output_File = os.path.join(output_dir, output_file)
        (job, output_file) = filter_mutect_job(vcf, pairing_file, output_file)
        maf_outputs.append(output_file)
        mutect_jobs.append(job)
    #FIXME TEMP DIR!?!?!?!?!?
    ###########DONE WITH MAF FIXING NOW MERGING AND EXTRA ANNOTATION JOBS
    #OUTPUT NAMES OF MERGE JOB
    merged_maf_name = os.path.join(output_dir, "merge_maf3")
    #MERGE JOB
    merge_job = merged_maf_job(maf_outputs, merged_maf_name)
    #OUTPUT NAMES OF TRINUC/IMPACT JOB 
    merged_seq = os.path.join(output_dir, "merged_maf3.seq")
    merged_impact_pos = os.path.join(output_dir, "merged_maf3.impact410")
    #TRINUC IMPACT JOB
    t_and_i_job = trinuc_and_impact_job(merged_maf_name, merged_seq, merged_impact_pos)
    #OUTPUT NAME OF MAF2MAF
    maf_w_vep = os.path.join(output_dir, "merge_maf3.vep")
    #maf2maf JOB
    maf2maf_job = maf2maf_job(merged_maf_name, maf_w_vep) 
    #FINAL OUTPUT NAME
    final_output = os.path.join(output_dir, project_id + "___SOMATIC.vep.maf")
    #ADD VARIANT INFO JOB
    final_maf_job = add_variant_info(merged_seq, merged_impact_pos, maf_w_vep, final_output)
    ###### dependencies
    job_deps = dict()
    job_deps[haplo_filter_job]=[merge_job]
    for job in mutect_jobs:
        job_deps[job]=[merge_job]
    job_deps[merge_job]=[t_and_i_job, maf2maf_job]
    job_deps[maf2maf_job]=[final_maf_job]
    job_deps[t_and_i_job]=[final_maf_job]
    all_jobs = [haplofilter_job, maf2maf_job, t_and_i_job, final_maf_job, merge_job]+mutect_jobs
    workflow.Workflow(all_jobs, job_deps, name="PostProcess:"+project_id)






    



      

if __name__=='__main__':
    parser = argparse.ArgumentParser(description="Run Facets on luna!", epilog="Include any FACETS args directly on this command line and they will be passed through")
    parser.add_argument("--pairing-file", required=True, help="The pairing filee")
    parser.add_argument("--pipeline-output-dir", required=True, help="The pipeline output directory")
    parser.add_argument("--project-id", help="Optionally override project ID dir name searching...")

    args = parser.parse_args()
    main(args.pairing_file, args.pipeline_output_dir, args.project_id)

  
         


