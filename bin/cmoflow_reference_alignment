#!/opt/common/CentOS_6-dev/python/python-2.7.10/bin/python
from cmo import workflow
import argparse, os, sys, re
import cmo, glob 
from collections import defaultdict
#WHOA, existentially troubling, man
PYTHON = cmo.util.programs['python']['default']
sample_map = {}


def check_configuration(file):
    #validate that we have enough information to run the pipeline
    return True

def construct_workflow(map_file, output_dir):
    alignment_directories = prepare_directories(map_file, output_dir)
    align_reads(alignment_directories)

def prepare_directories(map_file, output_dir):
    output_dir = os.path.join(output_dir, "intFiles/")
    fh = open(map_file)
    #FIXME copied the part about adding a "Count" for 
    #non unique ids from previous script that did this
    #mapping file has LIB, SAMPLE, FLOWCELL/RUNID, OUTUT DIRECTORY, SE/PE
    #make $output/sample_name/library/run_id directories for output
    #sometime that's not a unique identifier so check for duplicates and change name
    #symlink illumina directories to output directory
    slr_count = 0
    map_keys = ['library_id', 'sample_id', 'run_id', 'fastq_dir', 'pair_status']
    dirs_to_align = []
    while(1):
        line = fh.readline()
        if not line: 
            break
        map_values= line.split("\t")
        sample_dict = dict(zip(map_keys, map_values))
        sample_key = sample_dict['sample_id'] + sample_dict['library_id'] + sample_dict['run_id']
        if sample_key in sample_map:
            sample_key = sample_key + slr_count
            slr_count+=1
            sample_dict['run_id']=sample_dict['run_id']+"_" + str(slr_count)
        sample_map[sample_key] = sample_dict
    print sample_map
    for id, sample in sample_map.items():
        print output_dir, sample
        sample_output_dir = os.path.join(output_dir, sample['sample_id'], sample['library_id'], sample['run_id'])
        if not os.path.exists(sample_output_dir):
            os.makedirs(sample_output_dir)
        illumina_files = os.listdir(sample['fastq_dir'])
        for filename in illumina_files:
            if not os.path.exists(os.path.join(sample_output_dir,filename)):
                os.symlink(os.path.join(sample['fastq_dir'],filename),  os.path.join(sample_output_dir, filename))
        dirs_to_align.append(sample_output_dir)
    return dirs_to_align
    
#by the time we get here we should have symlinked all the fastqs into the directories we putting our bams and shit in

def sort_fastqs_into_dict(files):
    sorted = dict()
    paired_by_sample = defaultdict(list)
    for file in files:
        base = os.path.basename(file)
        m = re.search("(\S+)_(R[12])_(\d\d\d)", base)
        if not m.group(1) and m.group(2) and m.group(3):
            #FIXME LOGGING instead of CRITICAL fail?
            print >>sys.stderr, "Can't find filename parts (Sample/Barcode, R1/2, group) for this fastq: %s" % file
            sys.exit(1)
        #fastq file large sample and barcode prefix
        if m.group(1) not in sorted:
            sorted[m.group(1)]=dict()
        if m.group(3) not in sorted[m.group(1)]:
            sorted[m.group(1)][m.group(3)]=dict()
        sorted[m.group(1)][m.group(3)][m.group(2)]=file
    for sample in sorted:
        for readset in sorted[sample]:
            for read in ["R1", "R2"]:
                if read in sorted[sample][readset]:
                    paired_by_sample["_".join([sample,readset])].append(sorted[sample][readset][read])
    return paired_by_sample
            
            






def align_reads(alignment_directories):
    for directory in alignment_directories:
        read_pairs = sort_fastqs_into_dict(glob.glob(os.path.join(directory, "*R[12]*.fastq.gz")))
        #dictionary of key=FASTQ+BARCODE_ILLUMINA_SET value=LIST OF FASTQS
        jobs_list = []
        job_deps = {}
        #store split jobs separately for dependencies after we change granularity
        split_jobs = {}
        #store bams for merge
        bams = []
        #store bwa jobs for dependencies of merge
        bwa_jobs = []

        for readset, fastqs in read_pairs.items():
            readset_outputs = defaultdict(list)
            for fastq in fastqs:
                (unzip_fastq, gz_job) = gunzip_fastq(fastq)
                (unzip_cqs_fastq, cqs_job) = convert_quality_score(unzip_fastq)
                (split_reads, split_job) = split_reads(unzip_cqs_fastq)
                readset_outputs[readset].append(split_reads)
                jobs_list = jobs + [gz_job, cqs_job, split_job]
                job_deps[gz_job]=cqs_job
                job_deps[cqs_job]=split_job
                split_jobs[readset]=split_job
            for readset, fastqs in readset_outputs.items():
                #FIXME could be broken for Single end
                for i in range(0, len(fastqs[0])):
                    read1 = fastqs[0][i]
                    try:
                        read2 = fastqs[1][i]
                    except:
                        read2 = None
                    (cutadapt_read1, cut1_job) = cutadapt(read1)
                    (cutadapt_read2, cut2_job) = cutadapt(read2)
                    (bam, bwa_job) =  bwa(read1, read2)
                    bams.append(bam)
                    bwa_jobs.append(bwa_job)
                    jobs_list = jobs_list + [cut1_job, cut2_job, bwa_job]
                    job_deps[split_jobs[readset]]=cut1_job
                    job_deps[split_jobs[readset]]=cut2_job
                    job_deps[cut1_job]=bwa_job
                    job_deps[cut2_job]=bwa_job
            merge_job = picard_merge(bams)
            jobs_list = jobs_list + [merge_job]
            for job in bwa_jobs:
                job_deps[job] = merge_job
            #pipeline should be done!


def gunzip_fastq(fastq):
    (unzipped_fastq, ext) = os.path.splitext(fastq)

    cmd = ["zcat", fastq, ">", unzipped_fastq]
    gz_job = workflow.Job(" ".join(cmd), name="gunzip" + os.path.basename(unzipped_fastq))
    return(unzipped_fastq, gz_job)

def convert_quality_score(fastq):
    (basename, ext) = os.path.splitext(fastq)
    output = basename + "_CQS"
    cmd = ["ConvertQualityScore", "--input", fastq, "--output", output]
    cqs_job = workflow.Job(" ".join(cmd), name="CQS" + os.path.basename(fastq))
    return (output, cqs_job)

def split_reads(fastq):
    (basename, ext) = os.path.split










    #for each directory: find all fastqs
        #call convertquality score
        #call cutadapt on each fastq
        #split into groups of four million(?)
        #call bwa
        #merge to bam
    #return list of jobs and dependency hash for all these steps

def process_bams():
    pass


def mergeStats():
    pass

def generateGroupFile():
    pass



if __name__=='__main__':
    parser = argparse.ArgumentParser(description="Run Variant pipeline on luna!", epilog="supply options via a config file whose format is detailed at plvcbiocmo2.mskcc.org")
    parser.add_argument("--output-dir", help="output dir, will default to $CWD/TAG_NAME/")
    parser.add_argument("--config-file", help="configuration file")
    parser.add_argument("--map-file", help="file listing sample information for processing")
    parser.add_argument("--group-file", help="file listing grouping of samples for realign/recal steps")
    parser.add_argument("--genome", help="genome to align against", choices = cmo.util.genomes.keys())
    parser.add_argument("--pair-file", help="file listing tumor/normal pairs for mutect/maf conversion")
    parser.add_argument("--patient-file", help="if a patient file is given, patient wide fillout will be added to maf file")
    parser.add_argument("--workflow-mode", choices=["serial","LSF"], default="LSF", help="select 'serial' to run all jobs on the launching box. select 'LSF' to parallelize jobs as much as possible on luna")
    (args) = parser.parse_args()
    if args.output_dir:
        args.output_dir = os.path.abspath(args.output_dir)
    args_dict = vars(args)
    #validate required input file existence
    for key in ['pair_file', 'patient_file', 'map_file', 'group_file', 'config_file']:
        if key in args_dict and args_dict[key] != None:
            if not os.path.exists(args_dict[key]):
                pass
             #   print >>sys.stderr, "No file found for required argument: %s, value supplied: %s" % (key, args_dict[key])
            else:
                args_dict[key]=os.path.abspath(args_dict[key])
    #validate config
    check_configuration(args.config_file)
   

    (jobs, dependencies)  = construct_workflow(args.map_file, args.output_dir)
#    refalign_workflow = workflow.Workflow(jobs, dependencies, name=name)
#    refalign_workflow.run(args.workflow_mode)
         


