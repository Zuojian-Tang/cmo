#!/opt/common/CentOS_6-dev/python/python-2.7.10/bin/python
from cmo import workflow
import argparse, os, sys, re, csv
import cmo, glob, copy 
from collections import defaultdict
import subprocess, math
#WHOA, existentially troubling, man
PYTHON = cmo.util.programs['python']['default']
sample_map = {}
samplelib_grouping = {}
samplelib_sample = {}
r1_adaptor = 'AGATCGGAAGAGCACACGTCT';
r2_adaptor = 'AGATCGGAAGAGCGTCGTGTA';
dir2readgroup = {}
indels_1000g_gold_standard = "/ifs/work/socci/Pipelines/CBE/variants_pipeline/data/hg19/Mills_and_1000G_gold_standard.indels.hg19.vcf"
hapmap33 = "/ifs/work/socci/Pipelines/CBE/variants_pipeline/data/hg19/hapmap_3.3.hg19.vcf"
omni = "/ifs/work/socci/Pipelines/CBE/variants_pipeline/data/hg19/1000G_omni2.5.hg19.vcf"
snps_1000g_phase1_hc =  "/ifs/work/socci/Pipelines/CBE/variants_pipeline/data/hg19/1000G_phase1.snps.high_confidence.hg19.vcf"
dbsnp = "/ifs/depot/annotation/H.sapiens/dbSNP/v135/dbsnp_135.hg19__ReTag.vcf"
cosmic = "/ifs/work/socci/Pipelines/CBE/variants_pipeline/data/hg19/CosmicCodingMuts_v67_20131024.vcf"
project = "Project"
def add_sample_to_lib_grouping(sample_dict, sample_key):
    sample_id = sample_dict['sample_id']
    lib_id = sample_dict['library_id']
    library_key = sample_id + lib_id
    global samplelib_grouping
    if library_key not in samplelib_grouping:
        samplelib_grouping[sample_id+lib_id]=list()
    samplelib_grouping[library_key].append(sample_key)
    #FIXME this is a bit ugly to get a faster reference into it for process bams
    #TODO maybe refactor how smaple_map works so it works for both align_reads
    #and process bams
    if library_key not in samplelib_sample:
        samplelib_sample[library_key]=copy.deepcopy(sample_dict)
        #a library can have more than one readset so prepare to store thos
        samplelib_sample[library_key]['readset_id']=[sample_dict['readset_id']]
    else:
        samplelib_sample[library_key]['readset_id'].append(sample_dict['readset_id'][0])


def generate_group_file(sample_group_file, output_dir):
    fh = csv.reader(open(sample_group_file), delimiter="\t")
    outfile = os.path.join(output_dir, "bam_groupings.txt")
    ofh = csv.writer(open(outfile, "w"),
            delimiter="\t",
            quoting=csv.QUOTE_NONE)
    samples_in_group = defaultdict(list)
    bams_in_group = defaultdict(list)
    for [sample_name, group_id] in fh:
        if sample_name in sample_map:
            #make a list of all markdup bams for the group
            bams_in_group[group_id] = bams_in_group[group_id] + sample_map[sample_name]['md_bams']
            samples_in_group[group_id].append(sample_name)
        else:
            print >>sys.stderr, "Sample from group file not in map file!"
    for (group, sample_bams) in bams_in_group.items():
        print group, sample_bams
        ofh.writerow([group, ",".join(sample_bams), ",".join(samples_in_group[group])])
    return outfile 
    
        


def check_configuration(file):
    #validate that we have enough information to run the pipeline
    #FIXME do this
    return True

def construct_workflow(map_file, pair_file, group_file, output_dir, targets):
    alignment_directories = prepare_directories(map_file, output_dir)
    (align_jobs_list, align_job_deps, merged_bams, last_align_jobs) = align_reads(alignment_directories, output_dir)
    (process_jobs_list, process_job_deps, final_bams, final_sample_jobs, metric_jobs_and_files) = process_bams(merged_bams, output_dir, last_align_jobs, targets)
    (qc_job_list, qc_job_deps) = merge_stats(metric_jobs_and_files, output_dir, pair_file )
    mdbam_group_file = generate_group_file(group_file, output_dir)
    (variants_jobs, variant_job_deps) = fix_bam_and_call_variants(mdbam_group_file, pair_file, final_sample_jobs, output_dir)

    #FIXME move this into workflow or provide convenience function?
    for key in process_job_deps.keys():
        if key not in align_job_deps:
            align_job_deps[key]=process_job_deps[key]
        else:
            align_job_deps[key]=[align_job_deps[key], process_job_deps[key]]
    for key in qc_job_deps.keys():
        if key not in align_job_deps:
            align_job_deps[key]=qc_job_deps[key]
        else:
            align_job_deps[key]=[align_job_deps[key], qc_job_deps[key]]
    for key in variant_job_deps.keys():
        if key not in align_job_deps:
            align_job_deps[key]=variant_job_deps[key]
        else:
            if isinstance(align_job_deps[key], list):
                if not isinstance(variant_job_deps[key], list):
                    align_job_deps[key].append(variant_job_deps[key])
                else:
                    align_job_deps[key] = align_job_deps[key] + variant_job_deps[key]
            else:
                if not isinstance(variant_job_deps[key], list):
                    align_job_deps[key]=[align_job_deps[key], variant_job_deps[key]]
                else:
                    align_job_deps[key] = variant_job_deps[key].append(align_job_deps[key])

    merged_jobs_list = process_jobs_list + align_jobs_list + qc_job_list + variants_jobs

    return (merged_jobs_list, align_job_deps)


def fix_bam_and_call_variants(mdbam_group_file, pair_file, final_sample_jobs, output_dir):
    job_list = list()
    job_deps = dict()
    #RUN ABRA OR
    #FIXME WRITE ABRA SWITCH
    fh = open(mdbam_group_file, "rb")
    chrom_range = cmo.util.find_chromosomes(genome_string)
    all_sample_bams_by_chrom = defaultdict(list)
    print_reads_by_chrom = defaultdict(list)
    splitsamfiles_jobs = list()
    while(1):
        line = fh.readline()
        if not line:
            break
        #FIXME apparently this grouping file can specify the same sample multiple times
        #don't process the same sample twice, but not sure
        #which group it should preferentially bleong to then
        (group, bam_list, samples_in_group) = line.strip().split("\t")
        sample_list = samples_in_group.split(",")
        #FIXME no hardcode chroms :(
        realigned_chr_bams_for_group = list()
        realigner_target_jobs = list()
        indel_realigner_jobs = list()
        bams_by_chrom = defaultdict(dict)
        bam_list = bam_list.split(",")
        for chrom in chrom_range:
            (realigner_target_job, interval_file) = realigner_targets(group,chrom, bam_list, output_dir)
            (indel_realigner_job, realigned_bam) = indel_realigner(group, chrom, bam_list, interval_file, output_dir)
            bams_by_chrom['realign'][chrom]=realigned_bam
            realigned_chr_bams_for_group.append(realigned_bam)
            realigner_target_jobs.append(realigner_target_job)
            indel_realigner_jobs.append(indel_realigner_job)
            job_deps[realigner_target_job]=indel_realigner_job
            job_list = job_list + [realigner_target_job, indel_realigner_job]
        #hook up this new section the pipeline to the previous jobs in terms of job dependencies
        for sample, job in final_sample_jobs.items():
            if sample in sample_list:
                job_deps[job]=realigner_target_jobs 
        (base_recalibrator_job, bam_recal_matrix) = base_recalibrator(group, realigned_chr_bams_for_group, output_dir)
        for job in indel_realigner_jobs:
            job_deps[job]=base_recalibrator_job
        job_list.append(base_recalibrator_job)
        print_reads_jobs = list()
        for chrom in chrom_range:
            (print_reads_job, recal_bam) = print_reads(group, chrom, bam_recal_matrix,  bams_by_chrom['realign'][chrom], output_dir)
            bams_by_chrom['recal'][chrom]=recal_bam
            all_sample_bams_by_chrom[chrom].append(recal_bam)
            print_reads_jobs.append(print_reads_job)
            job_list.append(print_reads_job)
            print_reads_by_chrom[chrom].append(print_reads_job)
        (mergesamfiles_job, merged_bam) = merge_sam_files(group, bams_by_chrom['recal'], output_dir)
        job_list.append(mergesamfiles_job)
        (splitsamfile_job, by_sample_bams) = splitsamfiles(merged_bam, sample_list, output_dir) 
        job_deps[mergesamfiles_job]=splitsamfile_job
        job_list.append(splitsamfile_job)
        splitsamfiles_jobs.append(splitsamfile_job)
        for job in print_reads_jobs:
            job_deps[job]=[mergesamfiles_job]
        job_deps[base_recalibrator_job]=print_reads_jobs
    hc_vcfs = list()
    hc_jobs = list()
    #######RUN HAPLOTYPE CALLER BY CHROMOSOME, THEN VQSR
    for chrom in chrom_range:
        (haplotype_caller_job, hc_vcf) = haplotype_caller(chrom, all_sample_bams_by_chrom[chrom], output_dir)
        hc_vcfs.append(hc_vcf)
        hc_jobs.append(haplotype_caller_job)
        job_list.append(haplotype_caller_job)
        #multiple dependencies on print reads, append new dependency
        for job in print_reads_by_chrom[chrom]:
            job_deps[job].append(haplotype_caller_job)
    (combine_variants_job, merge_hc_vcf) = combine_variants(hc_vcfs, output_dir)
    for job in hc_jobs:
        job_deps[job]=combine_variants_job
    job_list.append(combine_variants_job)
    (hc_snp_recal_job, snp_recal_file,snps_tranches) = recalibrate_snps(merge_hc_vcf, output_dir)


    (hc_snp_apply_recal_job, snp_recal_vcf) = apply_recalibration_snps(merge_hc_vcf, snp_recal_file, snps_tranches, output_dir)
    (hc_indel_recal_job, indel_recal_file, indels_tranches) = recalibrate_indels(snp_recal_vcf, output_dir)
    (hc_indel_apply_recal_job, final_recal_vcf) = apply_recalibration_indels(snp_recal_vcf, indel_recal_file,indels_tranches, output_dir)
    job_deps[combine_variants_job]=[hc_snp_recal_job]
    job_deps[hc_snp_recal_job]=hc_snp_apply_recal_job
    job_deps[hc_snp_apply_recal_job] = hc_indel_recal_job
    job_deps[hc_indel_recal_job]=hc_indel_apply_recal_job
    job_list = job_list + [hc_snp_recal_job, hc_indel_recal_job, hc_snp_apply_recal_job, hc_indel_apply_recal_job]
    #####NOW DO PAIRED CALLING WITH MUTECt
    pair_csv = csv.reader(open(pair_file, "r"), delimiter="\t")
    for row in pair_csv:
        normal = row[0]
        tumor = row[1]
        if normal not in sample_map:
            print >>sys.stderr, "Normal sample %s not in sampl map, skipping row" % normal
            continue
        if tumor not in sample_map:
            print >>sys.stderr, "tumor sample %s not in sampl map, skipping row" % tumor
            continue
        #FIXME improve dependencies to match pairs to group (would be nice to have them in same file...)
        (mutect_job, mutect_vcf) = mutect(normal, tumor, output_dir)
        for job in splitsamfiles_jobs:
            if job in job_deps:
                job_deps[job].append(mutect_job)
            else:
                job_deps[job]=[mutect_job]
        job_list.append(mutect_job)
    return(job_list, job_deps)

def mutect(normal, tumor, output_dir):
    output_file = os.path.join(output_dir, "variants", "mutect", "_".join([normal, tumor, "mutect_calls.vcf"]))
    if not os.path.exists(os.path.dirname(output_file)):
        os.makedirs(os.path.dirname(output_file))
    cmd = ["cmo_mutect", "-R", genome_string,
            "--dbsnp", dbsnp,
            "--cosmic", cosmic,
            "--input_file:normal", sample_map[normal]['final_bam'],
            "--input_file:tumor", sample_map[tumor]['final_bam'],
            "--vcf", output_file,
            "--out", output_file.replace(".vcf", ".txt"),
            "-rf", "BadCigar", "--enable_extended_output", "--downsampling_type", "None"]
    job=workflow.Job(" ".join(cmd), name="Mutect " + tumor, resources="rusage[mem=4]", processors=2)
    return (job, output_file)
            





def recalibrate_snps(vcf, output_dir):
    output_file = os.path.join(output_dir, "intFiles", "HaplotypeCaller_SNP.recal")
    output_tranches = os.path.join(output_dir, "intFiles", "HaplotypeCaller_SNP.tranches")
    output_rscript = os.path.join(output_dir, "intFiles", "HaplotypeCaller_SNP.plots.R")
    cmd = [ "cmo_gatk",
            "-T", "VariantRecalibrator",
            "-R", genome_string,
            "-input", vcf,
            "-resource:hapmap,known=false,training=true,truth=true,prior=15.0", hapmap33,
            "-resource:omni,known=false,training=true,truth=true,prior=12.0", omni,
            "-resource:1000G,known=false,training=true,truth=false,prior=10.0", snps_1000g_phase1_hc,
            "-resource:dbsnp,known=true,training=false,truth=false,prior=2.0", dbsnp,
            "-an", "DP", "-an", "QD", "-an", "FS", "-an", "MQRankSum", 
            "-an", "ReadPosRankSum", "-mode", "SNP", "-tranche", "100.0",
            "-tranche", "99.0", "-tranche", "90.0", "-recalFile", output_file,
            "-tranchesFile", output_tranches,
            "-rscriptFile", output_rscript,
            "-nt", "4"]
    job = workflow.Job(" ".join(cmd), name="VQSR SNPS", processors=4, resources="rusage[mem=1]")
    return(job, output_file, output_tranches)

def recalibrate_indels(vcf, output_dir):
    output_file = os.path.join(output_dir, "intFiles", "HaplotypeCaller_INDEL.recal")
    output_tranches = os.path.join(output_dir, "intFiles", "HaplotypeCaller_INDEL.tranches")
    output_rscript = os.path.join(output_dir, "intFiles", "HaplotypeCaller_INDEL.plots.R")
    cmd = [ "cmo_gatk",
            "-T", "VariantRecalibrator",
            "-R", genome_string,
            "-input", vcf,
            "-resource:mills,known=false,training=true,truth=true,prior=12.0", indels_1000g_gold_standard,
            "-an", "DP", "-an", "FS", "-an", "MQRankSum", 
            "-an", "ReadPosRankSum", "-mode", "INDEL", "-tranche", "100.0",
            "-tranche", "99.9", "-tranche", "99.0", "-tranche", "90.0",
            "--maxGaussians", "4", "-recalFile", output_file,
            "-tranchesFile", output_tranches,
            "-rscriptFile", output_rscript,
            "-nt", "4"]
    job = workflow.Job(" ".join(cmd), name="VQSR INDELS", processors=4, resources="rusage[mem=1]")
    return(job, output_file, output_tranches)

def apply_recalibration_indels(vcf, recal_file, tranches_file, output_dir):
    output_file = os.path.join(output_dir, "variants", "haplotypecaller", "HaplotypeCaller.vcf")
    if not os.path.exists(os.path.dirname(output_file)):
            os.makedirs(os.path.dirname(output_file))
    cmd = ["cmo_gatk",
            "-T", "ApplyRecalibration",
            "-R", genome_string,
            "--ts_filter_level", "99.0",
            "-mode", "INDEL",
            "-tranchesFile", tranches_file, 
            "-recalFile", recal_file, 
            "-o", output_file,
            "-nt", "1",
            "-input", vcf]
    job = workflow.Job(" ".join(cmd), name="Apply SNP Recal", resources="rusage[mem=1]", processors=1)
    return (job, output_file)


def apply_recalibration_snps(vcf, recal_file, tranches_file, output_dir):
    output_file = os.path.join(output_dir, "intFiles", "HaplotypeCaller_SNP_vqsr.vcf")
    cmd = ["cmo_gatk",
            "-T", "ApplyRecalibration",
            "-R", genome_string,
            "--ts_filter_level", "99.0",
            "-mode", "SNP",
            "-tranchesFile", tranches_file, 
            "-recalFile", recal_file, 
            "-o", output_file,
            "-nt", "1"
            "-input", vcf]
    job = workflow.Job(" ".join(cmd), name="Apply SNP Recal", resources="rusage[mem=1]", processors=1)
    return (job, output_file)

    

def combine_variants(hc_vcfs, output_dir):
    output_file = os.path.join(output_dir, "intFiles", "HaplotypeCaller_RAW.vcf")
    cmd = ["cmo_gatk",
            "-T", "CombineVariants",
            "-R", genome_string,
            "-o", output_file,
            "--assumeIdenticalSamples"]
    for vcf in hc_vcfs:
        cmd = cmd + ["--variant", vcf]
    job = workflow.Job(" ".join(cmd), name="combine Haplo calls", resources="rusage[mem=2]", processors=1)
    return (job, output_file)


def haplotype_caller(chrom, bams, output_dir):
    output_file = os.path.join(output_dir, "intFiles", chrom+ "_HaplotypeCaller.vcf")
    cmd = ["cmo_gatk",
            "-T", "HaplotypeCaller",
            "-R ", genome_string,
            "-L", chrom,
            "--dbsnp", dbsnp,
            "--downsampling_type", "NONE",
            "--annotation", "AlleleBalanceBySample",
            "--annotation", "ClippingRankSumTest",
            "--read_filter", "BadCigar",
            "--num_cpu_threads_per_data_thread", "24",
            "--out", output_file,
            ]
    for bam in bams:
        cmd = cmd + ["-I", bam]
    job = workflow.Job(" ".join(cmd), name="Run HaplotypeCaller " + chrom, resources="rusage[mem=90]", processors=24)
    return (job, output_file)
   



def realigner_targets(group, chrom, bam_list, output_dir):
    interval_file = os.path.join(output_dir, "intFiles", "_".join([group, chrom, "indelRealigner.intervals"]))
    cmd = ["cmo_gatk",
            "-T", "RealignerTargetCreator",
            "-R", genome_string,
            "-L", chrom,
            "--known", indels_1000g_gold_standard,
            "--known", dbsnp,
            "-S", "LENIENT",
            "-nt", "10",
            "-rf", "BadCigar",
            "--downsampling_type", "NONE",
            "--out", interval_file,
            ]
    for bam in bam_list:
        cmd += ["-I", bam]
    job = workflow.Job(" ".join(cmd), name="Realigner group " + group + " chr " + chrom, resources="rusage[mem=5]", processors="10")
    return (job, interval_file)


def indel_realigner(group, chrom, bam_list, interval_file, output_dir):
    realigned_bam = os.path.join(output_dir, "intFiles", "_".join([group, chrom, "indelRealigned.bam"]))
    cmd = ["cmo_gatk",
            "-T", "IndelRealigner", 
            "-R", genome_string,
            "-L", chrom,
            "--knownAlleles", indels_1000g_gold_standard,
            "--knownAlleles", dbsnp,
            "-S", "LENIENT",
            "--targetIntervals", interval_file,
            "--maxReadsForRealignment", "500000",
            "--maxReadsInMemory", "3000000",
            "--maxReadsForConsensus", "500000",
            "-rf", "BadCigar",
            "--out", realigned_bam]
    for bam in bam_list:
        cmd += ["-I", bam]
    job = workflow.Job(" ".join(cmd), name="_".join(["IndelRealigner", group, chrom]), resources="rusage[mem=15]", processors="1") 
    return (job, realigned_bam)


def base_recalibrator(group, bams, output_dir):
    recal_matrix = os.path.join(output_dir, "intFiles", "_".join([group, "recal_data.grp"]))
    cmd = ["cmo_gatk",
            "-T", "BaseRecalibrator",
            "-l", "INFO",
            "-R", genome_string,
            "-S", "LENIENT",
            "--knownSites", dbsnp,
            "--knownSites", indels_1000g_gold_standard,
            "--knownSites", hapmap33,
            "--knownSites", snps_1000g_phase1_hc,
            "--covariate", "ContextCovariate",
            "--covariate", "CycleCovariate",
            "--covariate", "QualityScoreCovariate",
            "--covariate", "ReadGroupCovariate", 
            "-rf", "BadCigar",
            "--num_cpu_threads_per_data_thread", "12",
            "--out", recal_matrix]
    for bam in bams:
        cmd = cmd + ["-I", bam]
    job = workflow.Job(" ".join(cmd), resources="rusage[mem=40]", processors="12", name="Base Recalibration")
    return (job, recal_matrix)


def print_reads(group, chrom, bam_recal_matrix, input_bam, output_dir):
    out_bam = os.path.join(output_dir, "intFiles", "_".join([group, chrom, "indelRealigned_recal.bam"]))
    cmd = ["cmo_gatk",
            "-T", "PrintReads",
            "-R", genome_string,
            "-L", chrom,
            "--emit_original_quals",
            "-BQSR", bam_recal_matrix,
            "--num_cpu_threads_per_data_thread" , "6",
            "-rf", "BadCigar",
            "--downsampling_type", "NONE",
            "--out", out_bam,
            "-I", input_bam]
    job = workflow.Job(" ".join(cmd), name="_".join(["PrintReads", group, chrom]), resources="rusage[mem=30]", processors="6")
    return (job, out_bam)



#FIXME unify this call with picard_merge method
def merge_sam_files(group, bam_hash, output_dir):
    '''run picard merge'''
    #FIXME fix picard helper to accept default options instead of hiding them?
    out_bam = os.path.join(output_dir, "intFiles", "_".join([group,"indelRealigned_recal.bam"]))
    cmd = ["cmo_picard", "--cmd MergeSamFiles", "--SO coordinate", ]
    for bam in bam_hash.values():
        cmd = cmd + ["--I", bam]
    cmd = cmd + ["--O", out_bam]
    print " ".join(cmd)
    job = workflow.Job(" ".join(cmd), name="Merge bams",resources="rusage[iounits=6]")
    return (job, out_bam)
        

def splitsamfiles(merged_bam, sample_list, output_dir):
    out_prefix = os.path.join(output_dir, "alignments", "indelRealigned_recal_")
    if not os.path.exists(os.path.dirname(out_prefix)):
        os.makedirs(os.path.dirname(out_prefix))
    cmd = [ "cmo_gatk", 
            "-T", "SplitSamFile",
            "-R", genome_string,
            "-I", merged_bam, 
            "--outputRoot", out_prefix]
    by_sample_bams = list()
    for sample in sample_list:
        sample_bam = out_prefix + sample + ".bam"
        by_sample_bams.append(sample_bam)
        #FIXME global bullshit
        global sample_map
        sample_map[sample]['final_bam']=sample_bam
    job = workflow.Job(" ".join(cmd), name="Split Sam Files", resources="rusage[mem=10]", processors=1)
    return (job, by_sample_bams)

        
    #RUN REALIGNER TARGET CREATOR
    # AND RUN IndelRealigner in CHROMSOME PARALLELIZATION
    # RUN BASE RECALIBRATION


def prepare_directories(map_file, output_dir):
    output_dir = os.path.join(output_dir, "intFiles/")
    fh = open(map_file)
    #FIXME copied the part about adding a "Count" for 
    #non unique ids from previous script that did this
    #mapping file has LIB, SAMPLE, FLOWCELL/RUNID, OUTUT DIRECTORY, SE/PE
    #make $output/sample_name/library/run_id directories for output
    #sometime that's not a unique identifier so check for duplicates and change name
    #symlink illumina directories to output directory
    slr_count = 0
    map_keys = ['library_id', 'sample_id', 'run_id', 'fastq_dir', 'pair_status']
    dirs_to_align = []
    while(1):
        line = fh.readline()
        if not line: 
            break
        map_values= line.rstrip().split("\t")
        sample_dict = dict(zip(map_keys, map_values))
        sample_key = sample_dict['sample_id'] + sample_dict['library_id'] + sample_dict['run_id']
        if sample_key in sample_map:
            sample_key = sample_key + "_"+ str(slr_count)
            slr_count+=1
            sample_dict['run_id']=sample_dict['run_id']+"_" + str(slr_count)
        sample_dict['readset_id']=sample_key
        add_sample_to_lib_grouping(sample_dict, sample_key)
        sample_dict['readgroup']="\\t".join(
                ["@RG", 
                    "ID:" + sample_key + "_" + sample_dict['pair_status'],
                    "PL:Illumina",
                    "PU:" + sample_key,
                    "LB:" + sample_dict['sample_id'] + "_"+ sample_dict['library_id'],
                    "SM:" + sample_dict['sample_id']
                ])
        sample_dict['md_bams']=list()
        
        #sample_map[sample_key] = sample_dict
        sample_map[sample_dict['sample_id']]=sample_dict
    #print sample_map
    for id, sample in sample_map.items():
        sample_output_dir = os.path.join(output_dir, sample['sample_id'], sample['library_id'], sample['run_id'])
        if not os.path.exists(sample_output_dir):
            os.makedirs(sample_output_dir)
        illumina_files = os.listdir(sample['fastq_dir'])
        for filename in illumina_files:
            if not os.path.exists(os.path.join(sample_output_dir,filename)):
                os.symlink(os.path.join(sample['fastq_dir'],filename),  os.path.join(sample_output_dir, filename))
        dirs_to_align.append(sample_output_dir)
        dir2readgroup[sample_output_dir] = sample['readgroup']
    return dirs_to_align
    
#by the time we get here we should have symlinked all the fastqs into the directories we putting our bams and shit in

def sort_fastqs_into_dict(files):
    sorted = dict()
    readgroup_tags = dict()
    paired_by_sample = defaultdict(dict)
    for file in files:
        base = os.path.basename(file)
        m = re.search("([^_]+)_?(\S+)?_(\S+)_(R[12])_(\d\d\d).fastq.gz", base)
        print m.groups()
        if not m or not ( m.group(1) and m.group(4) and m.group(5)):
            #FIXME LOGGING instead of CRITICAL fail?
            print >>sys.stderr, "Can't find filename parts (Sample/Barcode, R1/2, group) for this fastq: %s" % file
            sys.exit(1)
        #fastq file large sample and barcode prefix
        readset = "_".join([m.group(1) + m.group(3)  + m.group(5)])
        if m.group(1) not in sorted:
            sorted[m.group(1)]=dict()
        if readset not in sorted[m.group(1)]:
            sorted[m.group(1)][readset]=dict()
        sorted[m.group(1)][readset][m.group(4)]=file
    for sample in sorted:
        for readset in sorted[sample]:
            for read in ["R1", "R2"]:
                if readset in paired_by_sample[sample]:
                    paired_by_sample[sample][readset].append(sorted[sample][readset][read])
                else:
                    paired_by_sample[sample][readset]=[sorted[sample][readset][read]]
    return paired_by_sample
            
            


def calculate_min_read_length(fastq):
    cmd = ["zcat", fastq, "| head -n 2 | tail -n 1 | wc -c"]
    print " ".join(cmd)
    read_length = subprocess.check_output(" ".join(cmd), shell=True, stderr=open(os.devnull,"w"))
    min_read_length = int(read_length.rstrip())/ 2
    print "Min Read Length: %d" % min_read_length
    return min_read_length


def cqs_metrics(root_dir):
    output_dir = os.path.join(root_dir, "metrics")
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    output_file = os.path.join(output_dir, "ConvertQualityScoreMetrics.txt")
    cmd = [PYTHON, cmo.util.programs['cqs_metrics']['default'],
            root_dir,
            '_cqs_metrics',
            output_file]
    return workflow.Job(" ".join(cmd), name="Merge CQS Metrics")

def cutadapt_metrics(root_dir):
    output_dir = os.path.join(root_dir, "metrics")
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    output_file = os.path.join(output_dir, "CutAdaptStats.txt")
    cmd = [PYTHON, cmo.util.programs['cutadapt_metrics']['default'],
                root_dir,
                "*CUTADAPT.stats",
                output_file]
    return workflow.Job(" ".join(cmd), name="Merge CutAdapt Stats")

    

def align_reads(alignment_directories, output_dir):
    merged_bams = {}
    final_align_jobs = {}
    jobs_list = []
    job_deps = {}
    cqs_metrics_job = cqs_metrics(output_dir)
    cutadapt_metrics_job = cutadapt_metrics(output_dir)
    jobs_list = [cqs_metrics_job, cutadapt_metrics_job]
    for directory in alignment_directories:
        read_pairs = sort_fastqs_into_dict(glob.glob(os.path.join(directory, "*R[12]*.fastq.gz")))
        #dictionary of key=FASTQ+BARCODE_ILLUMINA_SET value=LIST OF FASTQS
        #store split jobs separately for dependencies after we change granularity
        split_jobs = defaultdict(list)
        #store bams for merge
        min_read_length = None
        #GUNZIP, ConvertQuality Score, and split raw fastqs into 4million read chunks
        for sample, sample_readsets in read_pairs.items():
            #FIXME this god damn underscore dash hack 
            if sample not in sample_map:
                sample_map_key = sample.replace("-","_")
                if sample_map_key not in sample_map:
                    sample_map_key = "s_" + sample_map_key
                    if sample_map_key not in sample_map:
                        print >>sys.stderr, "can't lookup sample in internal sample_map, even with s_ and hyphern replacement: %s" % sample
                        print >>sys.stderr, "Sample map contents:"
                        print >>sys.stderr, sample_map
                        sys.exit(1)
            else:
                sample_map_key = sample
            bams = []
            bwa_jobs = []
            readset_id = sample_map[sample_map_key]['readset_id']
            for readset, fastqs in sample_readsets.items():
                readset_outputs = defaultdict(list)
                for fastq in fastqs:
                    if not min_read_length:
                        min_read_length=calculate_min_read_length(fastq)
                    (unzip_fastq, gz_job) = gunzip_fastq(fastq)
                    (unzip_cqs_fastq, cqs_job) = convert_quality_score(unzip_fastq)
                    (split_reads_files, split_job) = split_reads(unzip_cqs_fastq, fastq)
                    readset_outputs[readset].append(split_reads_files)
                    jobs_list = jobs_list + [gz_job, cqs_job, split_job]
                    job_deps[gz_job]=cqs_job
                    job_deps[cqs_job]=[split_job, cqs_metrics_job]
                    #store split jobs for dependencies)
                    split_jobs[readset].append(split_job)
                #store bwa jobs for dependencies of merge
                cut_jobs = defaultdict(list)
                for readset_chunk, fastqs in readset_outputs.items():
                    #FIXME could be broken for Single end
                    for i in range(0, len(fastqs[0])):
                        read1 = fastqs[0][i]
                        try:
                            read2 = fastqs[1][i]
                        except:
                            read2 = None
                        #original script uses two cutadapt jobs 
                        #cutadapt seems to support one pass
                        (cutadapt_read1, cutadapt_read2, cut_job) = cutadapt(read1, read2, min_read_length)
                        #recover readgroup we calculated earlier
                        #hacky, FIXME organize these two functions better
                        rg_tag = dir2readgroup[os.path.dirname(fastq)]
                        (bam, bwa_job) =  bwa(cutadapt_read1, cutadapt_read2, rg_tag)
                        bams.append(bam)
                        bwa_jobs.append(bwa_job)
                        cut_jobs[readset_chunk].append(cut_job)
                        jobs_list = jobs_list + [cut_job, bwa_job]
                        job_deps[cut_job]=[bwa_job, cutadapt_metrics_job]
                    for readset_chunk in readset_outputs.keys():
                        for split_job in split_jobs[readset_chunk]:
                            job_deps[split_job]=cut_jobs[readset_chunk]
                 #put final bam in the fastq directory!
            (merge_job, final_bam) = picard_merge(bams, readset_id, directory)
            jobs_list = jobs_list + [merge_job]
            for job in bwa_jobs:
                job_deps[job] = merge_job
            if readset_id not in merged_bams:
                merged_bams[readset_id]=[final_bam]
                final_align_jobs[readset_id]=[merge_job]
            else:
                merged_bams[readset_id].append(final_bam)
                final_align_jobs[readset_id].append(merge_job)
    return (jobs_list, job_deps, merged_bams, final_align_jobs)
                #pipeline should be done!


def gunzip_fastq(fastq):
    (unzipped_fastq, ext) = os.path.splitext(fastq)

    cmd = ["zcat", fastq, ">", unzipped_fastq]
    print " ".join(cmd)
    gz_job = workflow.Job(" ".join(cmd), name="gunzip " + os.path.basename(unzipped_fastq))
    return(unzipped_fastq, gz_job)

def convert_quality_score(fastq):
    (basename, ext) = os.path.splitext(fastq)
    output = basename + "_CQS"
    log = output + ".log"
    cmd = [cmo.util.programs['convertqualityscore']['default'], "--input", fastq, "--output", output, ">", log ]
    print " ".join(cmd)
    cqs_job = workflow.Job(" ".join(cmd), name="CQS " + os.path.basename(fastq))
    return (output, cqs_job)

def split_reads(cqs_fastq, orig_gzip_fastq):
    out_prefix=cqs_fastq + "__"
    cmd = ['/usr/bin/split', "-a 3", "-l 16000000", "-d", cqs_fastq, out_prefix]
    split_job = workflow.Job(" ".join(cmd), name="split "+os.path.basename(cqs_fastq))
    print " ".join(cmd)
    num_reads_cmd = ["zcat", orig_gzip_fastq, "|", "wc -l", "|", "cut -f 1"]
    print " ".join(num_reads_cmd)
    num_reads = subprocess.check_output(" ".join(num_reads_cmd), shell=True)
    num_files = math.ceil(float(num_reads) / 16000000)
    out_files = []
    for i in range(0, int(num_files)):
        out_files.append(out_prefix + "{:0>3d}".format(i))
    return (out_files, split_job)

def cutadapt(read1, read2, min_read_length):
    ''' run cutadapt '''
    #FIXME handle no read2
    read1_out = read1 + "_CT_PE.fastq"
    read2_out = read2 + "_CT_PE.fastq"
    stats_file = read1 + "_CUTADAPT.stats"
    cmd = [PYTHON, cmo.util.programs['cutadapt']['default'], 
            "-f fastq", "-a", r1_adaptor,
            "-A", r2_adaptor, "-O 10", "-m", str(min_read_length), 
            "-q 3", "--paired-output", read2_out, "-o", read1_out, 
            read1, read2,
            ">", stats_file]
    print " ".join(cmd)
    cut_job = workflow.Job(" ".join(cmd), name="cutadapt " + os.path.basename(read1_out))
    return (read1_out, read2_out, cut_job)
    
def bwa(read1, read2, readgroup):
    ''' run bwa mem '''
    #FIXME handle no read2
    #FIXME handle species and references
    #rg has tabs so make sure quoted in final command line
    bam = read1 + ".bwa.bam"
    cmd = ["cmo_bwa_mem", "--version default", "--genome", genome_string, "--fastq1", read1, "--fastq2", read2, "--output", bam, "-P", "-M", "-R", '"' + readgroup +'"', "-t 12"]
    #FIXME proper LSF resource request for bwa
    bwa_job =  workflow.Job(" ".join(cmd), resources="rusage[mem=12]", processors="12",name="bwa " + os.path.basename(bam))
    print " ".join(cmd)
    return(bam, bwa_job)

def samtools_index(bam):
    samtools = cmo.util.programs['samtools']['0.1.2']
    cmd = [ samtools, "index", bam ]
    return workflow.Job(" ".join(cmd), name="index " + os.path.basename(bam))

def picard_merge(bams, readset, directory):
    '''run picard merge'''
    #FIXME fix picard helper to accept default options instead of hiding them?
    out_bam = os.path.join(directory, readset+".bam")
    cmd = ["cmo_picard", "--cmd MergeSamFiles", "--SO coordinate", ]
    for bam in bams:
        cmd = cmd + ["--I", bam]
    cmd = cmd + ["--O", out_bam]
    print " ".join(cmd)
    picard_job = workflow.Job(" ".join(cmd), name="Merge bams " + readset,resources="rusage[iounits=6]")
    return (picard_job, out_bam)



def picard_markdups(bam, readset, directory):
    ''' run picard markdups '''
    #FIXME fix picard helper to accept default options instead of hiding them?
    out_bam = os.path.join(directory, readset +"_MD.bam")
    out_metrics = os.path.join(directory, readset + "_markDuplicatesMetrics.txt")
    cmd = ["cmo_picard", "--cmd MarkDuplicates"]
    cmd = cmd + ["--I", bam]
    cmd = cmd + ["--O", out_bam]
    cmd = cmd + ["--M", out_metrics]
    print " ".join(cmd)
    picard_job = workflow.Job(" ".join(cmd), name="Markdup " + readset,resources="rusage[mem=30]", processors=3)
    return (picard_job, out_bam, out_metrics)











def process_bams(merged_bams, output_dir, last_align_jobs, targets):
    ''' Take a list of bams that have been aligned in 4million read chunks and 
    merge them and deduplicate them on [sample+lib] basis
    tie them to the align section with the final merge job of each library
    '''
    output_dir = os.path.join(output_dir, "intFiles")
    jobs_list = list()
    job_deps = dict()
    by_sample_bams = defaultdict(list)
    by_sample_jobs = defaultdict(list)
    final_sample_bams = dict()
    final_sample_job = dict()
    metrics_jobs_and_files = defaultdict(list)
    dc_outs = list()
    #merge and deduplicate by sample, library
    for lib, readset_list in samplelib_grouping.items():
        bams = list()
        for readset in readset_list:
            bams = bams + merged_bams[readset]
        bylib_output_directory = os.path.join(output_dir, 
                samplelib_sample[lib]['sample_id'],
                samplelib_sample[lib]['library_id'],
                '')
        if not os.path.exists(bylib_output_directory):
            os.makedirs(bylib_output_directory)
        (lib_merge_job, merged_bam) = picard_merge(bams, lib, bylib_output_directory)    
        jobs_list.append(lib_merge_job)
        for readset in samplelib_sample[lib]['readset_id']:
            for job in last_align_jobs[readset]:
                job_deps[job]=lib_merge_job
        (markdup_job, mkdup_bam, metrics_file ) = picard_markdups(merged_bam, lib, bylib_output_directory)
        sample_map[samplelib_sample[lib]['sample_id']]['md_bams'].append(mkdup_bam)
        metrics_jobs_and_files['MarkDuplicateMetrics'].append((markdup_job, metrics_file))
        jobs_list.append(markdup_job)
        job_deps[lib_merge_job] = markdup_job
        sample_id = samplelib_sample[lib]['sample_id']
        by_sample_bams[sample_id].append(mkdup_bam)
        #can be more than one mkdup+index, if more than one library
        (index_job) = samtools_index(mkdup_bam)
        job_deps[markdup_job]=index_job
        jobs_list.append(index_job)
        by_sample_jobs[sample_id].append(index_job)
    #merge deduplicated library bams by sample
    for sample, bams in by_sample_bams.items():
        bysample_output_directory = os.path.join(output_dir,
                sample,
                '')
        if not os.path.exists(bysample_output_directory):
            os.makedirs(bysample_output_directory)
        (sample_merge_job, merged_bam) = picard_merge(bams, sample, bysample_output_directory)
        (index_job) = samtools_index(merged_bam)
        jobs_list.append(sample_merge_job)
        final_sample_job[sample]=sample_merge_job
        final_sample_bams[sample]=merged_bam
        for precursor_job in by_sample_jobs[sample]:
            job_deps[precursor_job]=sample_merge_job
        #next run a bunch of picard metric tools
    for sample, bam in final_sample_bams.items():
        (hsmetrics_job, hs_out) = hs_metrics(sample,bam, output_dir, targets)
        #FIXME only run if PE
        metrics_jobs_and_files['HsMetrics'].append((hsmetrics_job, hs_out))
        (insertsizemetrics_job, is_out, is_hist) = insertsize_metrics(sample,bam, output_dir)
        metrics_jobs_and_files['InsertSizeMetrics'].append((insertsizemetrics_job, is_out))
        metrics_jobs_and_files['InsertSizeMetrics_Histograms'].append((insertsizemetrics_job, is_hist))
        (alnsummetrics_job, as_out) = alignment_summary(sample,bam, output_dir)
        metrics_jobs_and_files['AlignmentSummaryMetrics'].append((alnsummetrics_job, as_out))
        (oxogmetrics_job, og_out) = oxog_metrics(sample,bam, output_dir)
        metrics_jobs_and_files['OxoGMetrics'].append((oxogmetrics_job, og_out))
        #FIXME only run if hg19?
        (depth_of_cov_job, dc_out) = gatk_doc(sample,bam, output_dir, targets)
        metrics_jobs_and_files['DOC'].append((depth_of_cov_job, dc_out))
        dc_outs.append(dc_out)
        job_deps[final_sample_job[sample]] =  [hsmetrics_job, insertsizemetrics_job, alnsummetrics_job, oxogmetrics_job, depth_of_cov_job]
        jobs_list = jobs_list + [hsmetrics_job, insertsizemetrics_job, alnsummetrics_job, oxogmetrics_job, depth_of_cov_job]

    return(jobs_list, job_deps, final_sample_bams, final_sample_job, metrics_jobs_and_files)

def merge_stats(metrics_jobs_and_files, root_dir, pair_file):
    output_dir  = os.path.join(root_dir, "metrics")
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    new_jobs = []
    new_job_deps = {}
    for stat_type in metrics_jobs_and_files.keys():
        jobs_and_files = metrics_jobs_and_files[stat_type]
        if stat_type == "DOC":
            (job, job_deps) = merge_gatk_metrics(jobs_and_files, pair_file, root_dir, output_dir)
        elif (stat_type=="InsertSizeMetrics_Histograms") :
            (job, job_deps) = merge_histograms(stat_type, jobs_and_files, root_dir, output_dir)
        else:
            (job, job_deps) = merge_picard_metrics(jobs_and_files, stat_type, output_dir)
        new_jobs.append(job)
        for (key, deps) in job_deps.items():
            if key in new_job_deps:
                new_job_deps[key]=[new_job_deps[key], job_deps[key]]
            else:
                new_job_deps[key]=job_deps[key]
    qc_job = qc_pdf(output_dir)
    for job in new_jobs:
        new_job_deps[job]=qc_job
    new_jobs.append(qc_job)
    return (new_jobs, new_job_deps) 

def qc_pdf(output_dir):
    cmd = ["cmo_qcpdf", "--metrics-directory", output_dir]
    return workflow.Job(" ".join(cmd), name="Generate QC PDF")
    #FIXME WRITE THIS SHIT

def merge_gatk_metrics(jobs_and_files, pair_file, root_dir, output_dir):
    output_dir = os.path.join(output_dir, "fingerprint")
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    cmd = [PYTHON, cmo.util.programs['analyzeFingerprint']['default'],
            "-pattern '*_FP_base_counts.txt'",
            "-pre TEST", 
            "-fp", cmo.util.targets['AgilentExon_51MB_hg19_v3']['FP_tiling_genotypes'] ,
            '-group', pair_file,
            '-groupType', "pairing",
            '-outdir', output_dir,
            '-dir', root_dir]
    new_job = workflow.Job(" ".join(cmd), name="Analyze Fingerprints")
    job_deps={}
    for (job, file) in jobs_and_files:
        job_deps[job]=new_job
    return (new_job, job_deps)

def merge_histograms(stat_type, jobs_and_files, root_dir, output_dir):
    output_file = os.path.join(output_dir, stat_type + ".txt")
    jobs = []
    files = []
    for (job, file) in jobs_and_files:
        jobs.append(job)
        files.append(file)
    cmd = [cmo.util.programs['mergeInsertSizeHistograms']['default'], 
            root_dir, 
            "InsertSizeMetrics_*.txt",
            output_file] 
    metrics_job = workflow.Job(" ".join(cmd), name="merge "  + stat_type)
    job_deps = dict()
    for job in jobs:
        job_deps[job] = metrics_job
    return (metrics_job, job_deps)


def merge_picard_metrics(jobs_and_files, stat_type, output_dir):
    output_file = os.path.join(output_dir, stat_type + ".txt")
    jobs = []
    files = []
    print jobs_and_files
    for (job, filename) in jobs_and_files:
        jobs.append(job)
        files.append(filename)
    cmd = [cmo.util.programs['mergePicardMetrics']['default'], 
            " ".join(files), 
            ">", output_file]
    metrics_job = workflow.Job(" ".join(cmd), name="merge "  + stat_type)
    job_deps = dict()
    for job in jobs:
        job_deps[job] = metrics_job
    return (metrics_job, job_deps)








        
def hs_metrics(sample,bam, output_dir, targets): 
    out = os.path.join(output_dir, "HsMetrics_" + sample + ".txt")
    cmd = ["cmo_picard", "--cmd CalculateHsMetrics"]
    cmd = cmd + ["--I", bam]
    cmd = cmd + ["--O", out]
    #FIXME need bait ilist from config]
    bait_ilist = cmo.util.targets[targets]['baits_ilist']
    cmd = cmd + ["--BI", bait_ilist]
    cmd = cmd + ["--LEVEL", "null"]
    cmd = cmd + ["--LEVEL", "SAMPLE"]
    bait_setname = targets
    cmd = cmd + ["--N" , bait_setname]
    targets_ilist = cmo.util.targets[targets]['targets_ilist']
    cmd = cmd + ["--TI", targets_ilist]
    print " ".join(cmd)
    metrics_job = workflow.Job(" ".join(cmd), name="HsMetrics " + sample,resources="rusage[mem=10]", processors=1)
    return (metrics_job, out)

def insertsize_metrics(sample,bam, output_dir):
    out = os.path.join(output_dir, "InsertSizeMetrics_" + sample + ".txt")
    outhist = os.path.join(output_dir, "InsertSizeMetrics_Histogram_" + sample + ".txt")
    cmd = ["cmo_picard", "--cmd CollectInsertSizeMetrics"]
    cmd = cmd + ["--I", bam]
    cmd = cmd + ["--O", out]
    cmd = cmd + ["--H", outhist]
    cmd = cmd + ["--LEVEL", "null"]
    cmd = cmd + ["--LEVEL", "SAMPLE"]
    print " ".join(cmd)
    metrics_job = workflow.Job(" ".join(cmd), name="InsertSizeMetrics " + sample,resources="rusage[mem=10]", processors=1)
    return (metrics_job, out, outhist)

def alignment_summary(sample,bam, output_dir):
    out = os.path.join(output_dir, "AlignmentSummaryMetrics_" + sample + ".txt")
    cmd = ["cmo_picard", "--cmd CollectAlignmentSummaryMetrics"]
    cmd = cmd + ["--I", bam]
    cmd = cmd + ["--O", out]
    cmd = cmd + ["--LEVEL", "null"]
    cmd = cmd + ["--LEVEL", "SAMPLE"]
    print " ".join(cmd)
    metrics_job = workflow.Job(" ".join(cmd), name="AlignmentSummary " + sample,resources="rusage[mem=10]", processors=3)
    return (metrics_job, out)

def oxog_metrics(sample,bam, output_dir):
    out = os.path.join(output_dir, "OxoGMetrics_" + sample + ".txt")
    cmd = ["cmo_picard", "--cmd CollectOxoGMetrics"]
    cmd = cmd + ["--I", bam]
    cmd = cmd + ["--O", out]
    #FIXME accept arbitrary refseq
    cmd = cmd + ["--R", genome_string]
    cmd = cmd + ["--DB_SNP", dbsnp]
    print " ".join(cmd)
    metrics_job = workflow.Job(" ".join(cmd), name="AlignmentSummary " + sample,resources="rusage[mem=10]", processors=3)
    return (metrics_job, out)

def gatk_doc(sample,bam, output_dir, targets):
    #FIXME fix GATK parser
    out = os.path.join(output_dir, bam + "_FP_base_counts.txt")
    cmd = ["cmo_gatk", "-T DepthOfCoverage"]
    cmd = cmd + ["-o", out]
    #FIXME accept arbitrary refseq
    cmd = cmd + ["-R", genome_string]
    cmd = cmd + ["-omitLocusTable", "-omitSampleSummary"]
    cmd = cmd + ["--includeRefNSites"]
    cmd = cmd + ["-I", bam]
    cmd = cmd + ["-L", cmo.util.targets[targets]['FP_intervals']]
    cmd = cmd + ["-rf", "BadCigar"]
    cmd = cmd + ["-mmq", "20", "-mbq", "0"]
    print " ".join(cmd)
    metrics_job = workflow.Job(" ".join(cmd), name="GATK DoC " + sample,resources="rusage[mem=10]", processors=3)
    return (metrics_job, out)



if __name__=='__main__':
    parser = argparse.ArgumentParser(description="Run Variant pipeline on luna!", epilog="supply options via a config file whose format is detailed at plvcbiocmo2.mskcc.org")
    parser.add_argument("--output-dir", help="output dir, will default to $CWD/TAG_NAME/")
    parser.add_argument("--config-file", help="configuration file")
    parser.add_argument("--map-file", help="file listing sample information for processing")
    parser.add_argument("--group-file", help="file listing grouping of samples for realign/recal steps")
    parser.add_argument("--pair-file", help="file listing tumor/normal pairs for mutect/maf conversion")
    parser.add_argument("--patient-file", help="if a patient file is given, patient wide fillout will be added to maf file")
    parser.add_argument("--project", help="name of project", default="RefAlign")
    parser.add_argument("--targets", choices=cmo.util.targets.keys(), required=True)
    parser.add_argument("--workflow-mode", choices=["serial","LSF"], default="LSF", help="select 'serial' to run all jobs on the launching box. select 'LSF' to parallelize jobs as much as possible on luna")
    parser.add_argument("--workflow-name", default="Reference alignment", help="name for this worklfow on GUI")
    parser.add_argument("--genome", choices=["GRCh37", "hg19"], default="GRCh37")
    (args) = parser.parse_args()
    if args.output_dir:
        args.output_dir = os.path.abspath(args.output_dir)
    args_dict = vars(args)
    #validate required input file existence
    for key in ['pair_file', 'patient_file', 'map_file', 'group_file', 'config_file']:
        if key in args_dict and args_dict[key] != None:
            if not os.path.exists(args_dict[key]):
                pass
             #   print >>sys.stderr, "No file found for required argument: %s, value supplied: %s" % (key, args_dict[key])
            else:
                args_dict[key]=os.path.abspath(args_dict[key])
    #validate config
    check_configuration(args.config_file)
    project=args.project
    genome_string = args.genome
    omni = cmo.util.genomes[args.genome]['omni']
    snps_1000g_phase1_hc = cmo.util.genomes[args.genome]['snps_1000g']
    dbsnp  = cmo.util.genomes[args.genome]['dbsnp']
    cosmic = cmo.util.genomes[args.genome]['cosmic']
    indels_1000g_gold_standard = cmo.util.genomes[args.genome]['indels_1000g']
    hapmap33 = cmo.util.genomes[args.genome]['hapmap33']





    (jobs, dependencies)  = construct_workflow(args.map_file, args.pair_file, args.group_file, args.output_dir, args.targets)
    refalign_workflow = workflow.Workflow(jobs, dependencies, name=args.workflow_name)
    refalign_workflow.run(args.workflow_mode)
         


