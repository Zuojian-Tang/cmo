#!/opt/common/CentOS_6-dev/python/python-2.7.10/bin/python
from cmo import workflow
import argparse, os, sys, re
import cmo, glob, copy 
from collections import defaultdict
import subprocess, math
#WHOA, existentially troubling, man
PYTHON = cmo.util.programs['python']['default']
sample_map = {}
samplelib_grouping = {}
samplelib_sample = {}
r1_adaptor = 'AGATCGGAAGAGCACACGTCT';
r2_adaptor = 'AGATCGGAAGAGCGTCGTGTA';
dir2readgroup = {}


def add_sample_to_lib_grouping(sample_dict, sample_key):
    sample_id = sample_dict['sample_id']
    lib_id = sample_dict['library_id']
    library_key = sample_id + lib_id
    global samplelib_grouping
    if library_key not in samplelib_grouping:
        samplelib_grouping[sample_id+lib_id]=list()
    samplelib_grouping[library_key].append(sample_key)
    #FIXME this is a bit ugly to get a faster reference into it for process bams
    #TODO maybe refactor how smaple_map works so it works for both align_reads
    #and process bams
    if library_key not in samplelib_sample:
        samplelib_sample[library_key]=copy.deepcopy(sample_dict)
        #a library can have more than one readset so prepare to store thos
        samplelib_sample[library_key]['readset_id']=[sample_dict['readset_id']]
    else:
        samplelib_sample[library_key]['readset_id'].append(sample_dict['readset_id'][0])



def check_configuration(file):
    #validate that we have enough information to run the pipeline
    #FIXME do this
    return True

def construct_workflow(map_file, pair_file, output_dir, targets):
    alignment_directories = prepare_directories(map_file, output_dir)
    (align_jobs_list, align_job_deps, merged_bams, last_align_jobs) = align_reads(alignment_directories, output_dir)
    (process_jobs_list, process_job_deps, final_bams, metric_jobs_and_files) = process_bams(merged_bams, output_dir, last_align_jobs, targets)
    (qc_job_list, qc_job_deps) = merge_stats(metric_jobs_and_files, output_dir, pair_file)
    #FIXME move this into workflow or provide convenience function?
    for key in process_job_deps.keys():
        if key not in align_job_deps:
            align_job_deps[key]=process_job_deps[key]
        else:
            align_job_deps[key]=[align_job_deps[key], process_job_deps[key]]
    for key in qc_job_deps.keys():
        if key not in align_job_deps:
            align_job_deps[key]=qc_job_deps[key]
        else:
            align_job_deps[key]=[align_job_deps[key], qc_job_deps[key]]
    merged_jobs_list = process_jobs_list + align_jobs_list + qc_job_list
    #for job in merged_jobs_list:
    #    print job.fw_id, job.name
    #for job, dep_job in align_job_deps.items():
    #    print job.fw_id, job.name, dep_job.fw_id, dep_job.name
    return (merged_jobs_list, align_job_deps)

def prepare_directories(map_file, output_dir):
    output_dir = os.path.join(output_dir, "intFiles/")
    fh = open(map_file)
    #FIXME copied the part about adding a "Count" for 
    #non unique ids from previous script that did this
    #mapping file has LIB, SAMPLE, FLOWCELL/RUNID, OUTUT DIRECTORY, SE/PE
    #make $output/sample_name/library/run_id directories for output
    #sometime that's not a unique identifier so check for duplicates and change name
    #symlink illumina directories to output directory
    slr_count = 0
    map_keys = ['library_id', 'sample_id', 'run_id', 'fastq_dir', 'pair_status']
    dirs_to_align = []
    while(1):
        line = fh.readline()
        if not line: 
            break
        map_values= line.rstrip().split("\t")
        sample_dict = dict(zip(map_keys, map_values))
        sample_key = sample_dict['sample_id'] + sample_dict['library_id'] + sample_dict['run_id']
        if sample_key in sample_map:
            sample_key = sample_key + "_"+ str(slr_count)
            slr_count+=1
            sample_dict['run_id']=sample_dict['run_id']+"_" + str(slr_count)
        sample_dict['readset_id']=sample_key
        add_sample_to_lib_grouping(sample_dict, sample_key)
        sample_dict['readgroup']="\\t".join(
                ["@RG", 
                    "ID:" + sample_key + "_" + sample_dict['pair_status'],
                    "PL:Illumina",
                    "PU:" + sample_key,
                    "LB:" + sample_dict['sample_id'] + "_"+ sample_dict['library_id'],
                    "SM:" + sample_dict['sample_id']
                ])
        
        #sample_map[sample_key] = sample_dict
        sample_map[sample_dict['sample_id']]=sample_dict
    #print sample_map
    for id, sample in sample_map.items():
        sample_output_dir = os.path.join(output_dir, sample['sample_id'], sample['library_id'], sample['run_id'])
        if not os.path.exists(sample_output_dir):
            os.makedirs(sample_output_dir)
        illumina_files = os.listdir(sample['fastq_dir'])
        for filename in illumina_files:
            if not os.path.exists(os.path.join(sample_output_dir,filename)):
                os.symlink(os.path.join(sample['fastq_dir'],filename),  os.path.join(sample_output_dir, filename))
        dirs_to_align.append(sample_output_dir)
        dir2readgroup[sample_output_dir] = sample['readgroup']
    return dirs_to_align
    
#by the time we get here we should have symlinked all the fastqs into the directories we putting our bams and shit in

def sort_fastqs_into_dict(files):
    sorted = dict()
    readgroup_tags = dict()
    paired_by_sample = defaultdict(dict)
    for file in files:
        base = os.path.basename(file)
        m = re.search("(\S+)_(\S+)_(\S+)(R[12])_(\d\d\d)", base)
        if not m.group(1) and m.group(2) and m.group(3):
            #FIXME LOGGING instead of CRITICAL fail?
            print >>sys.stderr, "Can't find filename parts (Sample/Barcode, R1/2, group) for this fastq: %s" % file
            sys.exit(1)
        #fastq file large sample and barcode prefix
        readset = "_".join([m.group(2) + m.group(3)  + m.group(5)])
        if m.group(1) not in sorted:
            sorted[m.group(1)]=dict()
        if readset not in sorted[m.group(1)]:
            sorted[m.group(1)][readset]=dict()
        sorted[m.group(1)][readset][m.group(4)]=file
    for sample in sorted:
        for readset in sorted[sample]:
            for read in ["R1", "R2"]:
                if readset in paired_by_sample[sample]:
                    paired_by_sample[sample][readset].append(sorted[sample][readset][read])
                else:
                    paired_by_sample[sample][readset]=[sorted[sample][readset][read]]
    return paired_by_sample
            
            


def calculate_min_read_length(fastq):
    cmd = ["zcat", fastq, "| head -n 2 | tail -n 1 | wc -c"]
    print " ".join(cmd)
    read_length = subprocess.check_output(" ".join(cmd), shell=True, stderr=open(os.devnull,"w"))
    min_read_length = int(read_length.rstrip())/ 2
    print "Min Read Length: %d" % min_read_length
    return min_read_length


def cqs_metrics(root_dir):
    output_dir = os.path.join(root_dir, "metrics")
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    output_file = os.path.join(output_dir, "ConvertQualityScoreMetrics.txt")
    cmd = [PYTHON, cmo.util.programs['cqs_metrics']['default'],
            root_dir,
            '_cqs_metrics',
            output_file]
    return workflow.Job(" ".join(cmd), name="Merge CQS Metrics")

def cutadapt_metrics(root_dir):
    output_dir = os.path.join(root_dir, "metrics")
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    output_file = os.path.join(output_dir, "CutAdaptStats.txt")
    cmd = [PYTHON, cmo.util.programs['cutadapt_metrics']['default'],
                root_dir,
                "*CUTADAPT.stats",
                output_file]
    return workflow.Job(" ".join(cmd), name="Merge CutAdapt Stats")

    

def align_reads(alignment_directories, output_dir):
    merged_bams = {}
    final_align_jobs = {}
    jobs_list = []
    job_deps = {}
    cqs_metrics_job = cqs_metrics(output_dir)
    cutadapt_metrics_job = cutadapt_metrics(output_dir)
    jobs_list = [cqs_metrics_job, cutadapt_metrics_job]
    for directory in alignment_directories:
        read_pairs = sort_fastqs_into_dict(glob.glob(os.path.join(directory, "*R[12]*.fastq.gz")))
        #dictionary of key=FASTQ+BARCODE_ILLUMINA_SET value=LIST OF FASTQS
        #store split jobs separately for dependencies after we change granularity
        split_jobs = defaultdict(list)
        #store bams for merge
        min_read_length = None
        #GUNZIP, ConvertQuality Score, and split raw fastqs into 4million read chunks
        for sample, sample_readsets in read_pairs.items():
            sample_map_key = "s_" + sample.replace("-","_")
            for readset, fastqs in sample_readsets.items():
                readset_outputs = defaultdict(list)
                for fastq in fastqs:
                    if not min_read_length:
                        min_read_length=calculate_min_read_length(fastq)
                    (unzip_fastq, gz_job) = gunzip_fastq(fastq)
                    (unzip_cqs_fastq, cqs_job) = convert_quality_score(unzip_fastq)
                    (split_reads_files, split_job) = split_reads(unzip_cqs_fastq, fastq)
                    readset_outputs[readset].append(split_reads_files)
                    jobs_list = jobs_list + [gz_job, cqs_job, split_job]
                    job_deps[gz_job]=cqs_job
                    job_deps[cqs_job]=[split_job, cqs_metrics_job]
                    #store split jobs for dependencies)
                    split_jobs[readset].append(split_job)
                bams = []
                #store bwa jobs for dependencies of merge
                cut_jobs = defaultdict(list)
                bwa_jobs = []
                for readset_chunk, fastqs in readset_outputs.items():
                    #FIXME could be broken for Single end
                    for i in range(0, len(fastqs[0])):
                        read1 = fastqs[0][i]
                        try:
                            read2 = fastqs[1][i]
                        except:
                            read2 = None
                        #original script uses two cutadapt jobs 
                        #cutadapt seems to support one pass
                        (cutadapt_read1, cutadapt_read2, cut_job) = cutadapt(read1, read2, min_read_length)
                        #recover readgroup we calculated earlier
                        #hacky, FIXME organize these two functions better
                        rg_tag = dir2readgroup[os.path.dirname(fastq)]
                        (bam, bwa_job) =  bwa(cutadapt_read1, cutadapt_read2, rg_tag)
                        bams.append(bam)
                        bwa_jobs.append(bwa_job)
                        cut_jobs[readset_chunk].append(cut_job)
                        jobs_list = jobs_list + [cut_job, bwa_job]
                        job_deps[cut_job]=[bwa_job, cutadapt_metrics_job]
                    for readset_chunk in readset_outputs.keys():
                        for split_job in split_jobs[readset_chunk]:
                            job_deps[split_job]=cut_jobs[readset_chunk]
                    #put final bam in the fastq directory!
                (merge_job, final_bam) = picard_merge(bams, sample_map[sample_map_key]['readset_id'], directory)
                jobs_list = jobs_list + [merge_job]
                for job in bwa_jobs:
                    job_deps[job] = merge_job
                
                merged_bams[sample_map[sample_map_key]['readset_id']]=final_bam
                final_align_jobs[sample_map[sample_map_key]['readset_id']]=merge_job
    return (jobs_list, job_deps, merged_bams, final_align_jobs)
                #pipeline should be done!


def gunzip_fastq(fastq):
    (unzipped_fastq, ext) = os.path.splitext(fastq)

    cmd = ["zcat", fastq, ">", unzipped_fastq]
    print " ".join(cmd)
    gz_job = workflow.Job(" ".join(cmd), name="gunzip " + os.path.basename(unzipped_fastq))
    return(unzipped_fastq, gz_job)

def convert_quality_score(fastq):
    (basename, ext) = os.path.splitext(fastq)
    output = basename + "_CQS"
    log = output + ".log"
    cmd = [cmo.util.programs['convertqualityscore']['default'], "--input", fastq, "--output", output, ">", log ]
    print " ".join(cmd)
    cqs_job = workflow.Job(" ".join(cmd), name="CQS " + os.path.basename(fastq))
    return (output, cqs_job)

def split_reads(cqs_fastq, orig_gzip_fastq):
    out_prefix=cqs_fastq + "__"
    cmd = ['/usr/bin/split', "-a 3", "-l 16000000", "-d", cqs_fastq, out_prefix]
    split_job = workflow.Job(" ".join(cmd), name="split "+os.path.basename(cqs_fastq))
    print " ".join(cmd)
    num_reads_cmd = ["zcat", orig_gzip_fastq, "|", "wc -l", "|", "cut -f 1"]
    print " ".join(num_reads_cmd)
    num_reads = subprocess.check_output(" ".join(num_reads_cmd), shell=True)
    num_files = math.ceil(float(num_reads) / 16000000)
    out_files = []
    for i in range(0, int(num_files)):
        out_files.append(out_prefix + "{:0>3d}".format(i))
    return (out_files, split_job)

def cutadapt(read1, read2, min_read_length):
    ''' run cutadapt '''
    #FIXME handle no read2
    read1_out = read1 + "_CT_PE.fastq"
    read2_out = read2 + "_CT_PE.fastq"
    stats_file = read1 + "_CUTADAPT.stats"
    cmd = [PYTHON, cmo.util.programs['cutadapt']['default'], 
            "-f fastq", "-a", r1_adaptor,
            "-A", r2_adaptor, "-O 10", "-m", str(min_read_length), 
            "-q 3", "--paired-output", read2_out, "-o", read1_out, 
            read1, read2,
            ">", stats_file]
    print " ".join(cmd)
    cut_job = workflow.Job(" ".join(cmd), name="cutadapt " + os.path.basename(read1_out))
    return (read1_out, read2_out, cut_job)
    
def bwa(read1, read2, readgroup):
    ''' run bwa mem '''
    #FIXME handle no read2
    #FIXME handle species and references
    #rg has tabs so make sure quoted in final command line
    bam = read1 + ".bwa.bam"
    cmd = ["cmo_bwa_mem", "--version default", "--genome hg19", "--fastq1", read1, "--fastq2", read2, "--output", bam, "-P", "-M", "-R", '"' + readgroup +'"', "-t 12"]
    #FIXME proper LSF resource request for bwa
    bwa_job =  workflow.Job(" ".join(cmd), resources="rusage[mem=12]", processors="12",name="bwa " + os.path.basename(bam))
    print " ".join(cmd)
    return(bam, bwa_job)

def picard_merge(bams, readset, directory):
    '''run picard merge'''
    #FIXME fix picard helper to accept default options instead of hiding them?
    out_bam = os.path.join(directory, readset+".bam")
    cmd = ["cmo_picard", "--cmd MergeSamFiles", "--SO coordinate", ]
    for bam in bams:
        cmd = cmd + ["--I", bam]
    cmd = cmd + ["--O", out_bam]
    print " ".join(cmd)
    picard_job = workflow.Job(" ".join(cmd), name="Merge bams " + readset,resources="rusage[iounits=6]")
    return (picard_job, out_bam)



def picard_markdups(bam, readset, directory):
    ''' run picard markdups '''
    #FIXME fix picard helper to accept default options instead of hiding them?
    out_bam = os.path.join(directory, readset +"_MD.bam")
    out_metrics = os.path.join(directory, readset + "_markDuplicatesMetrics.txt")
    cmd = ["cmo_picard", "--cmd MarkDuplicates"]
    cmd = cmd + ["--I", bam]
    cmd = cmd + ["--O", out_bam]
    cmd = cmd + ["--M", out_metrics]
    print " ".join(cmd)
    picard_job = workflow.Job(" ".join(cmd), name="Markdup " + readset,resources="rusage[mem=30]", processors=3)
    return (picard_job, out_bam, out_metrics)











def process_bams(merged_bams, output_dir, last_align_jobs, targets):
    ''' Take a list of bams that have been aligned in 4million read chunks and 
    merge them and deduplicate them on [sample+lib] basis
    tie them to the align section with the final merge job of each library
    '''
    output_dir = os.path.join(output_dir, "intFiles")
    jobs_list = list()
    job_deps = dict()
    by_sample_bams = defaultdict(list)
    by_sample_jobs = defaultdict(list)
    final_sample_bams = dict()
    final_sample_job = dict()
    metrics_jobs_and_files = defaultdict(list)
    dc_outs = list()
    #merge and deduplicate by sample, library
    for lib, readset_list in samplelib_grouping.items():
        bams = list()
        for readset in readset_list:
            bams.append(merged_bams[readset])
        bylib_output_directory = os.path.join(output_dir, 
                samplelib_sample[lib]['sample_id'],
                samplelib_sample[lib]['library_id'],
                '')
        if not os.path.exists(bylib_output_directory):
            os.makedirs(bylib_output_directory)
        (lib_merge_job, merged_bam) = picard_merge(bams, lib, bylib_output_directory)    
        jobs_list.append(lib_merge_job)
        for readset in samplelib_sample[lib]['readset_id']:
            job_deps[last_align_jobs[readset]]=lib_merge_job
        (markdup_job, mkdup_bam, metrics_file ) = picard_markdups(merged_bam, lib, bylib_output_directory)
        metrics_jobs_and_files['MarkDuplicateMetrics'].append((markdup_job, metrics_file))
        jobs_list.append(markdup_job)
        job_deps[lib_merge_job] = markdup_job
        sample_id = samplelib_sample[lib]['sample_id']
        by_sample_bams[sample_id].append(mkdup_bam)
        by_sample_jobs[sample_id].append(markdup_job)
    #merge deduplicated library bams by sample
    for sample, bams in by_sample_bams.items():
        bysample_output_directory = os.path.join(output_dir,
                sample,
                '')
        if not os.path.exists(bysample_output_directory):
            os.makedirs(bysample_output_directory)
        (sample_merge_job, merged_bam) = picard_merge(bams, sample, bysample_output_directory)
        jobs_list.append(sample_merge_job)
        final_sample_job[sample]=sample_merge_job
        final_sample_bams[sample]=merged_bam
        for precursor_job in by_sample_jobs[sample]:
            job_deps[precursor_job]=sample_merge_job
        #next run a bunch of picard metric tools
    for sample, bam in final_sample_bams.items():
        (hsmetrics_job, hs_out) = hs_metrics(sample,bam, output_dir, targets)
        #FIXME only run if PE
        metrics_jobs_and_files['HsMetrics'].append((hsmetrics_job, hs_out))
        (insertsizemetrics_job, is_out, is_hist) = insertsize_metrics(sample,bam, output_dir)
        metrics_jobs_and_files['InsertSizeMetrics'].append((insertsizemetrics_job, is_out))
        metrics_jobs_and_files['InsertSizeMetrics_Histograms'].append((insertsizemetrics_job, is_hist))
        (alnsummetrics_job, as_out) = alignment_summary(sample,bam, output_dir)
        metrics_jobs_and_files['AlignmentSummaryMetrics'].append((alnsummetrics_job, as_out))
        (oxogmetrics_job, og_out) = oxog_metrics(sample,bam, output_dir)
        metrics_jobs_and_files['OxoGMetrics'].append((oxogmetrics_job, og_out))
        #FIXME only run if hg19?
        (depth_of_cov_job, dc_out) = gatk_doc(sample,bam, output_dir)
        metrics_jobs_and_files['DOC'].append((depth_of_cov_job, dc_out))
        dc_outs.append(dc_out)
        job_deps[final_sample_job[sample]] =  [hsmetrics_job, insertsizemetrics_job, alnsummetrics_job, oxogmetrics_job, depth_of_cov_job]
        jobs_list = jobs_list + [hsmetrics_job, insertsizemetrics_job, alnsummetrics_job, oxogmetrics_job, depth_of_cov_job]

    return(jobs_list, job_deps, final_sample_bams, metrics_jobs_and_files)

def merge_stats(metrics_jobs_and_files, root_dir, pair_file):
    output_dir  = os.path.join(root_dir, "metrics")
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    new_jobs = []
    new_job_deps = {}
    for stat_type in metrics_jobs_and_files.keys():
        jobs_and_files = metrics_jobs_and_files[stat_type]
        if stat_type == "DOC":
            (job, job_deps) = merge_gatk_metrics(jobs_and_files, pair_file, root_dir, output_dir)
        elif (stat_type=="InsertSizeMetrics_Histograms") :
            (job, job_deps) = merge_histograms(stat_type, jobs_and_files, root_dir, output_dir)
        else:
            (job, job_deps) = merge_picard_metrics(jobs_and_files, stat_type, output_dir)
        new_jobs.append(job)
        for (key, deps) in job_deps.items():
            if key in new_job_deps:
                new_job_deps[key]=[new_job_deps[key], job_deps[key]]
            else:
                new_job_deps[key]=job_deps[key]
    qc_job = qc_pdf(output_dir)
    for job in new_jobs:
        new_job_deps[job]=qc_job
    new_jobs.append(qc_job)
    return (new_jobs, new_job_deps) 

def qc_pdf(output_dir):
    cmd = ["cmo_qcpdf", "--metrics-directory", output_dir]
    return workflow.Job(" ".join(cmd), name="Generate QC PDF")
    #FIXME WRITE THIS SHIT

def merge_gatk_metrics(jobs_and_files, pair_file, root_dir, output_dir):
    output_dir = os.path.join(output_dir, "fingerprint")
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    cmd = [PYTHON, cmo.util.programs['analyzeFingerprint']['default'],
            "-pattern '*_FP_base_counts.txt'",
            "-pre TEST", 
            "-fp", cmo.util.targets['AgilentExon_51MB_hg19_v3']['tiling_genotypes'] ,
            '-group', pair_file,
            '-groupType', "pairing",
            '-outdir', output_dir,
            '-dir', root_dir]
    new_job = workflow.Job(" ".join(cmd), name="Analyze Fingerprints")
    job_deps={}
    for (job, file) in jobs_and_files:
        job_deps[job]=new_job
    return (new_job, job_deps)

def merge_histograms(stat_type, jobs_and_files, root_dir, output_dir):
    output_file = os.path.join(output_dir, stat_type + ".txt")
    jobs = []
    files = []
    for (job, file) in jobs_and_files:
        jobs.append(job)
        files.append(file)
    cmd = [cmo.util.programs['mergeInsertSizeHistograms']['default'], 
            root_dir, 
            "InsertSizeMetrics_*.txt",
            output_file] 
    metrics_job = workflow.Job(" ".join(cmd), name="merge "  + stat_type)
    job_deps = dict()
    for job in jobs:
        job_deps[job] = metrics_job
    return (metrics_job, job_deps)


def merge_picard_metrics(jobs_and_files, stat_type, output_dir):
    output_file = os.path.join(output_dir, stat_type + ".txt")
    jobs = []
    files = []
    print jobs_and_files
    for (job, filename) in jobs_and_files:
        jobs.append(job)
        files.append(filename)
    cmd = [cmo.util.programs['mergePicardMetrics']['default'], 
            " ".join(files), 
            ">", output_file]
    metrics_job = workflow.Job(" ".join(cmd), name="merge "  + stat_type)
    job_deps = dict()
    for job in jobs:
        job_deps[job] = metrics_job
    return (metrics_job, job_deps)








        
def hs_metrics(sample,bam, output_dir, targets): 
    out = os.path.join(output_dir, "HsMetrics_" + sample + ".txt")
    cmd = ["cmo_picard", "--cmd CalculateHsMetrics"]
    cmd = cmd + ["--I", bam]
    cmd = cmd + ["--O", out]
    #FIXME need bait ilist from config]
    bait_ilist = cmo.util.targets[targets]['baits_ilist']
    cmd = cmd + ["--BI", bait_ilist]
    cmd = cmd + ["--LEVEL", "null"]
    cmd = cmd + ["--LEVEL", "SAMPLE"]
    bait_setname = targets
    cmd = cmd + ["--N" , bait_setname]
    targets_ilist = cmo.util.targets[targets]['targets_ilist']
    cmd = cmd + ["--TI", targets_ilist]
    print " ".join(cmd)
    metrics_job = workflow.Job(" ".join(cmd), name="HsMetrics " + sample,resources="rusage[mem=10]", processors=1)
    return (metrics_job, out)

def insertsize_metrics(sample,bam, output_dir):
    out = os.path.join(output_dir, "InsertSizeMetrics_" + sample + ".txt")
    outhist = os.path.join(output_dir, "InsertSizeMetrics_Histogram_" + sample + ".txt")
    cmd = ["cmo_picard", "--cmd CollectInsertSizeMetrics"]
    cmd = cmd + ["--I", bam]
    cmd = cmd + ["--O", out]
    cmd = cmd + ["--H", outhist]
    cmd = cmd + ["--LEVEL", "null"]
    cmd = cmd + ["--LEVEL", "SAMPLE"]
    print " ".join(cmd)
    metrics_job = workflow.Job(" ".join(cmd), name="InsertSizeMetrics " + sample,resources="rusage[mem=10]", processors=1)
    return (metrics_job, out, outhist)

def alignment_summary(sample,bam, output_dir):
    out = os.path.join(output_dir, "AlignmentSummaryMetrics_" + sample + ".txt")
    cmd = ["cmo_picard", "--cmd CollectAlignmentSummaryMetrics"]
    cmd = cmd + ["--I", bam]
    cmd = cmd + ["--O", out]
    cmd = cmd + ["--LEVEL", "null"]
    cmd = cmd + ["--LEVEL", "SAMPLE"]
    print " ".join(cmd)
    metrics_job = workflow.Job(" ".join(cmd), name="AlignmentSummary " + sample,resources="rusage[mem=10]", processors=3)
    return (metrics_job, out)

def oxog_metrics(sample,bam, output_dir):
    out = os.path.join(output_dir, "OxoGMetrics_" + sample + ".txt")
    cmd = ["cmo_picard", "--cmd CollectOxoGMetrics"]
    cmd = cmd + ["--I", bam]
    cmd = cmd + ["--O", out]
    #FIXME accept arbitrary refseq
    cmd = cmd + ["--R", "hg19"]
    cmd = cmd + ["--DB_SNP", "/ifs/depot/annotation/H.sapiens/dbSNP/v135/dbsnp_135.hg19__ReTag.vcf"]
    print " ".join(cmd)
    metrics_job = workflow.Job(" ".join(cmd), name="AlignmentSummary " + sample,resources="rusage[mem=10]", processors=3)
    return (metrics_job, out)

def gatk_doc(sample,bam, output_dir):
    #FIXME fix GATK parser
    out = os.path.join(output_dir, bam + "_FP_base_counts.txt")
    cmd = ["cmo_gatk", "-T DepthOfCoverage"]
    cmd = cmd + ["-o", out]
    #FIXME accept arbitrary refseq
    cmd = cmd + ["-R", "hg19"]
    cmd = cmd + ["-omitLocusTable", "-omitSampleSummary"]
    cmd = cmd + ["-includeRefNSites"]
    cmd = cmd + ["-I", bam]
    cmd = cmd + ["-rf", "BadCigar"]
    cmd = cmd + ["-mmq", "20", "-mbq", "0"]
    print " ".join(cmd)
    metrics_job = workflow.Job(" ".join(cmd), name="GATK DoC " + sample,resources="rusage[mem=10]", processors=3)
    return (metrics_job, out)







def generateGroupFile():
    pass



if __name__=='__main__':
    parser = argparse.ArgumentParser(description="Run Variant pipeline on luna!", epilog="supply options via a config file whose format is detailed at plvcbiocmo2.mskcc.org")
    parser.add_argument("--output-dir", help="output dir, will default to $CWD/TAG_NAME/")
    parser.add_argument("--config-file", help="configuration file")
    parser.add_argument("--map-file", help="file listing sample information for processing")
    parser.add_argument("--group-file", help="file listing grouping of samples for realign/recal steps")
    parser.add_argument("--genome", help="genome to align against", choices = cmo.util.genomes.keys())
    parser.add_argument("--pair-file", help="file listing tumor/normal pairs for mutect/maf conversion")
    parser.add_argument("--patient-file", help="if a patient file is given, patient wide fillout will be added to maf file")
    parser.add_argument("--targets", choices=cmo.util.targets.keys(), required=True)
    parser.add_argument("--workflow-mode", choices=["serial","LSF"], default="LSF", help="select 'serial' to run all jobs on the launching box. select 'LSF' to parallelize jobs as much as possible on luna")
    (args) = parser.parse_args()
    if args.output_dir:
        args.output_dir = os.path.abspath(args.output_dir)
    args_dict = vars(args)
    #validate required input file existence
    for key in ['pair_file', 'patient_file', 'map_file', 'group_file', 'config_file']:
        if key in args_dict and args_dict[key] != None:
            if not os.path.exists(args_dict[key]):
                pass
             #   print >>sys.stderr, "No file found for required argument: %s, value supplied: %s" % (key, args_dict[key])
            else:
                args_dict[key]=os.path.abspath(args_dict[key])
    #validate config
    check_configuration(args.config_file)


    (jobs, dependencies)  = construct_workflow(args.map_file, args.pair_file, args.output_dir, args.targets)
    refalign_workflow = workflow.Workflow(jobs, dependencies, name="refalign test")
    refalign_workflow.run(args.workflow_mode)
         


